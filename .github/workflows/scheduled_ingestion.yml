  # Name of the workflow, which will appear in the GitHub Actions tab
name: Scheduled Daily Data Ingestion

# This section defines the trigger for the workflow
on:
  # This allows you to run the workflow manually from the Actions tab for testing
  workflow_dispatch:

  # This is the schedule trigger
  schedule:
    # This is a CRON expression that means 'run at 7:00 AM UTC every day'
    - cron: '0 7 * * *'

# This section defines the jobs to be run
jobs:
  # We only have one job, which we'll call 'ingest-daily-data'
  ingest-daily-data:
    # The type of virtual machine to run the job on. 'ubuntu-latest' is a reliable default.
    runs-on: ubuntu-latest
    
    # The sequence of steps the job will perform
    steps:
      # Step 1: Checks out your repository's code onto the runner
      - name: Checkout repository
        uses: actions/checkout@v4

      # Step 2: Sets up the Python environment
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      # Step 3: Installs the Python packages listed in your requirements.txt file
      - name: Install dependencies
        run: pip install -r requirements.txt

      # Step 4: Authenticates with Google Cloud using a secret
      - name: Authenticate to Google Cloud
        uses: 'google-github-actions/auth@v2'
        with:
          credentials_json: '${{ secrets.GCP_CREDENTIALS }}' # Ensure your secret is named GCP_CREDENTIALS

      # Step 5: Run the ingestion script with all necessary environment variables
      - name: Run Python ingestion script
        env:
          GCP_PROJECT_ID: "gs-arial"
          BIGQUERY_DATASET_ID: "financial_data_landing"
          BIGQUERY_TABLE_ID: "stock_values"
          STOCK_SYMBOL_TO_FETCH: "IBM"
          ALPHA_VANTAGE_API_KEY: ${{ secrets.ALPHA_VANTAGE_API_KEY }}
        run: python data_ingestion/ingest_shares.py